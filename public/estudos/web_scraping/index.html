<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/2025-2-SeLiga/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=2025-2-SeLiga/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Web Scraping | SeLiga</title>
<meta name="keywords" content="">
<meta name="description" content="
O que é Web Scraping?
Web scraping refere-se ao processo de extração(mineração) de conteúdo e dados de sites usando software. Nós devemos usar esta ferramenta para minerar as notícias dos jornais, para assim podermos usá-las. Por meio de scripts e robôs, essa metodologia permite a coleta, estruturação e análise de dados que não estão disponíveis via APIs.
No nosso caso, não precisamos tomar o cuidado com o uso &ldquo;mal-intencionado&rdquo; para dados sigilosos, pois nosso foco é o uso de dados públicos fornecidos pelos jornais. Mas devemos tomar o cuidado para não causarmos outro tipo de web scraping &ldquo;mal-intencionado&rdquo;, que é o &quot;over-scraping&quot;, onde os scrapers enviam muitas solicitações em um determinado período. Muitas solicitações podem sobrecarregar os provedores de serviços de internet.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/2025-2-SeLiga/estudos/web_scraping/">
<link crossorigin="anonymous" href="/2025-2-SeLiga/assets/css/stylesheet.bf5658f8b68defa6da5d91a2ecdf95130a49e863ffadf2cd7632e70e4151ebac.css" integrity="sha256-v1ZY&#43;LaN76baXZGi7N&#43;VEwpJ6GP/rfLNdjLnDkFR66w=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/2025-2-SeLiga/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/2025-2-SeLiga/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/2025-2-SeLiga/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/2025-2-SeLiga/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/2025-2-SeLiga/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/2025-2-SeLiga/estudos/web_scraping/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="/2025-2-SeLiga/css/custom.min.css">
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/2025-2-SeLiga/" accesskey="h" title="SeLiga (Alt + H)">SeLiga</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/2025-2-SeLiga/inicio/" title="Início">
                    <span>Início</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/2025-2-SeLiga/equipe" title="Equipe">
                    <span>Equipe</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/2025-2-SeLiga/documentacao" title="Documentação">
                    <span>Documentação</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/2025-2-SeLiga/menuestudos" title="Estudos">
                    <span>Estudos</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Web Scraping
    </h1>
    <div class="post-meta">

</div>
  </header> 
  <div class="post-content"><hr>
<h2 id="o-que-é-web-scraping">O que é Web Scraping?<a hidden class="anchor" aria-hidden="true" href="#o-que-é-web-scraping">#</a></h2>
<p><code>Web scraping</code> refere-se ao processo de extração(mineração) de conteúdo e dados de sites usando software. Nós devemos usar esta ferramenta para minerar as notícias dos jornais, para assim podermos usá-las. Por meio de scripts e robôs, essa metodologia permite a coleta, estruturação e análise de dados que não estão disponíveis via <code>APIs</code>.</p>
<p>No nosso caso, não precisamos tomar o cuidado com o uso &ldquo;mal-intencionado&rdquo; para dados sigilosos, pois nosso foco é o uso de dados públicos fornecidos pelos jornais. Mas devemos tomar o cuidado para não causarmos outro tipo de <code>web scraping</code> &ldquo;mal-intencionado&rdquo;, que é o <code>&quot;over-scraping&quot;</code>, onde os scrapers enviam muitas solicitações em um determinado período. Muitas solicitações podem sobrecarregar os provedores de serviços de internet.</p>
<p>Nos casos mais simples, <code>web scraping</code> pode ser feita através de uma <code>API</code> de um site ou interface de programação de aplicativos. Quando um site disponibiliza sua <code>API</code>, os desenvolvedores web podem usá-la para extrair automaticamente dados e outras informações úteis em um formato conveniente.
É claro que nem sempre é assim, e muitos sites que se quer extrair não terão uma <code>API</code> que você possa usar. Além disso, mesmo os sites que têm uma <code>API</code> nem sempre fornecem dados no formato correto.
Como resultado, o <code>web scraping</code> é necessário apenas quando os dados da web que você quer não estão disponíveis na(s) forma(s) que você precisa. Seja porque os formatos que você quer não estão disponíveis, ou o site simplesmente não está fornecendo o escopo completo de dados, o <code>web scraping</code> torna possível obter o que você quer.</p>
<h1 id="como-funciona-o-web-scraping">Como funciona o Web Scraping?<a hidden class="anchor" aria-hidden="true" href="#como-funciona-o-web-scraping">#</a></h1>
<p>Embora os métodos e ferramentas possam variar, tudo o que você precisa fazer é encontrar uma maneira de <code>(1)</code> navegar automaticamente no(s) site(s) de destino e <code>(2)</code> extrair os dados uma vez que estiver lá. Geralmente, essas etapas são realizadas com o uso de <code>scrapers</code> e <code>crawlers</code>.</p>
<h2 id="scrapers-e-crawlers">Scrapers e Crawlers<a hidden class="anchor" aria-hidden="true" href="#scrapers-e-crawlers">#</a></h2>
<p>Podemos usar uma analogia para entender como funciona. Por exemplo, um cavalo e um arado:</p>
<p>Assim como o cavalo guia o arado, o arado revira e fragmenta a terra, ajudando a abrir caminho para novas sementes enquanto reintegra plantas daninhas indesejadas e resíduos de culturas ao solo.
O <code>web scraping</code> não é muito diferente. Aqui, um <code>crawler</code> desempenha o papel do cavalo, guiando o <code>scraper</code> por nossos campos digitais. Logo, nosso scraper é nosso arado.</p>
<ul>
<li><code>Crawler</code> (às vezes conhecidos como <code>spiders</code>): Navega sistematicamente pela internet para indexar e descobrir páginas. Ele segue os hiperlinks de uma página para outra, mapeando a estrutura do site.</li>
<li><code>Scraper</code>: É a ferramenta que, uma vez em uma página específica, extrai os dados desejados do seu conteúdo <code>HTML</code>.</li>
</ul>
<h3 id="ferramentas-ideais-para-minerar-conteúdo-de-jornais">Ferramentas Ideais para Minerar Conteúdo de Jornais<a hidden class="anchor" aria-hidden="true" href="#ferramentas-ideais-para-minerar-conteúdo-de-jornais">#</a></h3>
<p>Para este projeto, a combinação de ferramentas precisa lidar com a complexidade dos portais de notícias modernos.</p>
<ul>
<li>
<p><code>Scrapy</code>: É a ferramenta mais indicada para este cenário. Sua capacidade de gerenciar múltiplos crawlers de forma assíncrona é perfeita para monitorar dezenas de jornais simultaneamente. Ele pode ser configurado para revisitar sites periodicamente em busca de novos artigos.</p>
</li>
<li>
<p><code>Selenium ou Playwright</code>: Muitos portais de notícias carregam artigos dinamicamente ou possuem <code>&quot;paywalls&quot;</code> parciais que aparecem após a rolagem. O Selenium é essencial para simular o comportamento humano, rolar a página e garantir que todo o conteúdo do artigo seja carregado antes da extração.</p>
</li>
<li>
<p><code>Beautiful Soup</code> <em>(em conjunto com <code>Requests</code>)</em>: Para portais de notícias mais simples e com estrutura <code>HTML estática</code>, esta combinação é suficiente e mais rápida de implementar para extrair os dados de um único artigo.</p>
</li>
<li>
<p><code>Newspaper3k</code> <em>(Biblioteca Python)</em>: Uma biblioteca de alto nível construída especificamente para extração e curadoria de artigos de notícias. Com um simples <code>URL</code>, ela pode extrair automaticamente o título, autores, data de publicação e o texto principal, economizando muito tempo na fase de desenvolvimento do <code>scraper</code>.</p>
</li>
</ul>
<hr>
<h1 id="processo-de-extração-de-dados-geral">Processo de extração de dados Geral<a hidden class="anchor" aria-hidden="true" href="#processo-de-extração-de-dados-geral">#</a></h1>
<p>Após a inicialização e preparação do ambiente de <code>Banco de Dados</code> deve-se:</p>
<ol>
<li>Coletar <code>URLs</code> de artigos de notícias das páginas iniciais dos portais. Cada portal possui uma &ldquo;aranha&rdquo; implementada usando a <code>biblioteca Scrapy</code>, localizada no <code>spiders/diretório</code>.</li>
<li>Rastreamento (<code>Crawling</code>): O <code>crawler</code> navegará pelas seções principais dos jornais (ex: &ldquo;Política&rdquo;, &ldquo;Economia&rdquo;, &ldquo;Mundo&rdquo;) para encontrar links para novos artigos. O objetivo é descobrir e listar os <code>URLs</code> das notícias publicadas.</li>
<li>Requisição e Parsing: Para cada <code>URL</code> de notícia, o scraper fará uma requisição <code>HTTP</code> para obter o conteúdo <code>HTML</code> da página.</li>
<li>Extração de Elementos-Chave: Esta é a etapa mais crítica. O scraper é programado para &ldquo;ler&rdquo; o <code>HTML</code> e extrair:</li>
</ol>
<ul>
<li><em>O Título da Notícia:</em> Geralmente encontrado em tags <code>&lt;h1&gt;</code> ou <code>&lt;h2&gt;</code>.</li>
<li><em>O Corpo do Texto:</em> O conteúdo principal do artigo, normalmente dentro de uma série de parágrafos <code>&lt;p&gt;</code>.</li>
<li><em>O Autor e a Data de Publicação:</em> Metadados essenciais para análise de credibilidade e contexto.</li>
<li><em>Links e Fontes Citadas:</em> Identificar links externos no corpo do texto pode ajudar a verificar as fontes da matéria.</li>
<li><em>Imagens e Legendas:</em> Podem ser usadas para busca reversa de imagens, uma técnica comum na checagem de fatos.</li>
</ul>
<ol start="5">
<li>Armazenamento para Análise: Os dados extraídos (título, texto, autor, data, fonte) são salvos de forma estruturada (ex: em um <code>banco de dados</code> ou <code>JSON</code>), prontos para serem processados pelo seu sistema de classificação.</li>
</ol>
<hr>
<h1 id="bibliotecas-úteis-para-implementação">Bibliotecas úteis para implementação<a hidden class="anchor" aria-hidden="true" href="#bibliotecas-úteis-para-implementação">#</a></h1>
<h2 id="requests">requests<a hidden class="anchor" aria-hidden="true" href="#requests">#</a></h2>
<p>Essa é uma biblioteca <code>Python</code> para fazer requisições <code>HTTP</code> de forma simples e elegante. Ela não interpreta ou renderiza o <code>HTML</code>, apenas o busca no servidor e o entrega como texto puro.</p>
<p>Ela é o ponto de partida de quase todo projeto de <code>web scraping</code>. Sua função é &ldquo;pedir&rdquo; a uma página da web o seu conteúdo, exatamente como um navegador faz quando você digita um endereço.
Deve ser usada o para obter o código-fonte <code>(HTML)</code> de qualquer página da web que seja estática (ou seja, que não precise de interações ou JavaScript para carregar seu conteúdo principal).</p>
<h3 id="exemplo">Exemplo:<a hidden class="anchor" aria-hidden="true" href="#exemplo">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># URL do site que queremos acessar</span>
</span></span><span style="display:flex;"><span>url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://exemplo.com&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Faz a requisição GET para a URL</span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(url)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Verifica se a requisição foi bem-sucedida (código 200)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> response<span style="color:#f92672">.</span>status_code <span style="color:#f92672">==</span> <span style="color:#ae81ff">200</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Imprime o conteúdo HTML da página</span>
</span></span><span style="display:flex;"><span>    print(response<span style="color:#f92672">.</span>text)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Falha na requisição. Código: </span><span style="color:#e6db74">{</span>response<span style="color:#f92672">.</span>status_code<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h2 id="beautifulsoup">BeautifulSoup<a hidden class="anchor" aria-hidden="true" href="#beautifulsoup">#</a></h2>
<p>É uma biblioteca para <code>parsing</code> de documentos <code>HTML</code> e <code>XML</code>. Ela pega o texto <code>HTML</code> bruto (obtido com o <code>requests</code>, por exemplo) e o transforma em uma estrutura de objetos <code>Python</code> navegável, como uma árvore.</p>
<p>Serve para encontrar, navegar e extrair informações de dentro do <code>HTML</code>. Com ela, você pode facilmente selecionar elementos por suas tags (ex: <code>&lt;p&gt;</code>, <code>&lt;h1&gt;</code>, <code>&lt;a&gt;</code>), classes <code>CSS</code> ou <code>IDs</code>.</p>
<p>Devemos usar logo após usar o <code>requests</code>. É a ferramenta perfeita para extrair dados de <code>HTML estático</code>. É extremamente popular por ser muito intuitiva e flexível.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> bs4 <span style="color:#f92672">import</span> BeautifulSoup
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://exemplo.com&#34;</span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(url)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Cria um objeto BeautifulSoup para &#34;parsear&#34; o HTML</span>
</span></span><span style="display:flex;"><span>soup <span style="color:#f92672">=</span> BeautifulSoup(response<span style="color:#f92672">.</span>text, <span style="color:#e6db74">&#39;html.parser&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Encontra o elemento &lt;h1&gt; e extrai o texto dele</span>
</span></span><span style="display:flex;"><span>titulo <span style="color:#f92672">=</span> soup<span style="color:#f92672">.</span>find(<span style="color:#e6db74">&#39;h1&#39;</span>)<span style="color:#f92672">.</span>text
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Título da página: </span><span style="color:#e6db74">{</span>titulo<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Encontra o primeiro parágrafo &lt;p&gt;</span>
</span></span><span style="display:flex;"><span>paragrafo <span style="color:#f92672">=</span> soup<span style="color:#f92672">.</span>find(<span style="color:#e6db74">&#39;p&#39;</span>)<span style="color:#f92672">.</span>text
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Parágrafo: </span><span style="color:#e6db74">{</span>paragrafo<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h2 id="lxml">lxml<a hidden class="anchor" aria-hidden="true" href="#lxml">#</a></h2>
<p>Esta é outra biblioteca de parsing para <code>HTML</code> e <code>XML</code>, assim como o <code>BeautifulSoup</code>. A grande diferença é que ela é um wrapper sobre bibliotecas escritas em <code>C</code>, o que a torna extremamente rápida e eficiente no processamento de documentos, mesmo os muito grandes ou malformados.</p>
<p>Serve para a mesma finalidade do <code>BeautifulSoup</code> (navegar e extrair dados), mas com um desempenho muito superior. Ela oferece suporte nativo a <code>XPath</code>, uma linguagem poderosa para selecionar nós em documentos <code>XML/HTML</code>, algo que o <code>BeautifulSoup</code> só faz com a ajuda de bibliotecas adicionais. O <code>BeautifulSoup</code> pode usar o lxml como seu &ldquo;motor&rdquo; de <code>parsing</code>, combinando a sintaxe amigável do <code>BeautifulSoup</code> com a velocidade do <code>lxml</code>.</p>
<h3 id="quando-usar">Quando usar?<a hidden class="anchor" aria-hidden="true" href="#quando-usar">#</a></h3>
<ol>
<li>Quando a velocidade de extração é crítica (ex: ao processar milhares de páginas).</li>
<li>Ao lidar com <code>HTML</code> malformado, pois <code>lxml</code> é muito robusto.</li>
<li>Você pode instalá-la <code>(pip install lxml)</code> e usá-la com o <code>BeautifulSoup</code> para obter o melhor dos dois mundos: a simplicidade do <code>BeautifulSoup</code> com a velocidade do <code>lxml</code>.</li>
</ol>
<h3 id="exemplo-1">Exemplo:<a hidden class="anchor" aria-hidden="true" href="#exemplo-1">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#75715e"># Primeiro, instale o lxml: pip install lxml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> bs4 <span style="color:#f92672">import</span> BeautifulSoup
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://exemplo.com&#34;</span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(url)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Note que agora usamos &#39;lxml&#39; como o parser</span>
</span></span><span style="display:flex;"><span>soup <span style="color:#f92672">=</span> BeautifulSoup(response<span style="color:#f92672">.</span>text, <span style="color:#e6db74">&#39;lxml&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># A sintaxe para encontrar elementos continua a mesma do BeautifulSoup</span>
</span></span><span style="display:flex;"><span>titulo <span style="color:#f92672">=</span> soup<span style="color:#f92672">.</span>find(<span style="color:#e6db74">&#39;h1&#39;</span>)<span style="color:#f92672">.</span>text
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Título da página (usando lxml): </span><span style="color:#e6db74">{</span>titulo<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h2 id="selenium">Selenium<a hidden class="anchor" aria-hidden="true" href="#selenium">#</a></h2>
<p>É uma ferramenta de automação de navegador. O Selenium não lê apenas o <code>HTML</code>; ele controla um navegador web real (como <code>Chrome</code> ou <code>Firefox</code>), que pode executar <code>JavaScript</code>, clicar em botões, preencher formulários e rolar a página.</p>
<p><em>Serve para extrair dados de sites dinâmicos.</em> Muitos sites modernos (redes sociais, portais de notícias, e-commerces) carregam seu conteúdo principal usando <code>JavaScript</code> depois que a página inicial é carregada. O requests não consegue ver esse conteúdo, mas o <code>Selenium</code> sim, pois ele espera o navegador renderizar tudo.</p>
<h3 id="quando-usar-1">Quando usar?<a hidden class="anchor" aria-hidden="true" href="#quando-usar-1">#</a></h3>
<ol>
<li>Quando o conteúdo que você quer extrair não aparece no <code>HTML</code> inicial (o que você obtém com <code>requests</code>).</li>
<li>Quando você precisa interagir com a página: clicar em um botão &ldquo;Ver mais&rdquo;, fazer <code>login</code>, selecionar uma opção em um menu, etc.</li>
<li>Para sites que usam &ldquo;rolagem infinita&rdquo; para carregar mais conteúdo.</li>
</ol>
<h3 id="exemplo-2">Exemplo:<a hidden class="anchor" aria-hidden="true" href="#exemplo-2">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#75715e"># Primeiro, instale o selenium e o driver do navegador (ex: chromedriver)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># pip install selenium</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> selenium <span style="color:#f92672">import</span> webdriver
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> selenium.webdriver.common.by <span style="color:#f92672">import</span> By
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inicializa o navegador (neste caso, Chrome)</span>
</span></span><span style="display:flex;"><span>driver <span style="color:#f92672">=</span> webdriver<span style="color:#f92672">.</span>Chrome()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Abre uma página que usa JavaScript</span>
</span></span><span style="display:flex;"><span>driver<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;https://www.exemplo-dinamico.com&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Espera 5 segundos para o JavaScript carregar o conteúdo</span>
</span></span><span style="display:flex;"><span>time<span style="color:#f92672">.</span>sleep(<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Encontra um elemento pelo seu ID (que pode ter sido criado por JS)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># A sintaxe para encontrar elementos é diferente do BeautifulSoup</span>
</span></span><span style="display:flex;"><span>elemento_dinamico <span style="color:#f92672">=</span> driver<span style="color:#f92672">.</span>find_element(By<span style="color:#f92672">.</span>ID, <span style="color:#e6db74">&#34;conteudo-gerado-por-js&#34;</span>)
</span></span><span style="display:flex;"><span>print(elemento_dinamico<span style="color:#f92672">.</span>text)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fecha o navegador</span>
</span></span><span style="display:flex;"><span>driver<span style="color:#f92672">.</span>quit()
</span></span></code></pre></div><h2 id="scrapy">Scrapy<a hidden class="anchor" aria-hidden="true" href="#scrapy">#</a></h2>
<p>É um framework completo de <code>Web Scraping</code> e <code>Crawling</code>. Ele não é apenas uma biblioteca, mas um ambiente de desenvolvimento inteiro para criar <code>&quot;spiders&quot;</code> (aranhas), que são robôs que podem navegar por um site, seguir links e extrair dados de forma estruturada.</p>
<p>Para construir projetos de <code>scraping</code> grandes, complexos e escaláveis. O <code>Scrapy</code> gerencia todo o fluxo de trabalho para você: faz as requisições (de forma assíncrona, ou seja, várias ao mesmo tempo), processa as respostas, extrai os dados através de seletores (<code>XPath</code> e <code>CSS</code>) e os salva no formato que você desejar (<code>JSON</code>, <code>CSV</code>, etc.).</p>
<h3 id="quando-usar-2">Quando usar?<a hidden class="anchor" aria-hidden="true" href="#quando-usar-2">#</a></h3>
<ol>
<li>
<p>Para projetos de larga escala: quando você precisa extrair dados de um site inteiro, ou de múltiplos sites.</p>
</li>
<li>
<p>Quando você precisa de desempenho: sua arquitetura assíncrona o torna muito mais rápido que a combinação <code>requests + BeautifulSoup</code> para múltiplas páginas.</p>
</li>
<li>
<p>Quando você precisa de um <em>pipeline de dados</em>: o <code>Scrapy</code> permite que você processe os itens extraídos, limpe os dados e os salve em um banco de dados, tudo dentro do mesmo projeto.</p>
</li>
</ol>
<h3 id="como-funciona-conceitualmente">Como funciona (conceitualmente):<a hidden class="anchor" aria-hidden="true" href="#como-funciona-conceitualmente">#</a></h3>
<p>Diferente das outras, você não escreve um script simples. Você cria um projeto <code>Scrapy</code> com uma estrutura de arquivos definida:</p>
<ul>
<li>
<p><code>spiders/</code>: Onde você define sua &ldquo;aranha&rdquo;, dizendo qual <code>URL</code> começar, como seguir os links e como extrair os dados de cada página.</p>
</li>
<li>
<p><code>items.py</code>: Onde você define a estrutura dos dados que quer extrair (ex: um produto com nome, preco, descricao).</p>
</li>
<li>
<p><code>pipelines.py</code>: Onde você define o que fazer com os dados extraídos (ex: limpar, validar, salvar no banco de dados).</p>
</li>
<li>
<p><code>settings.py</code>: Onde você configura o comportamento do seu robô (ex: a velocidade das requisições, headers, etc.).</p>
</li>
</ul>
<h2 id="resumo">Resumo<a hidden class="anchor" aria-hidden="true" href="#resumo">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Biblioteca</th>
          <th style="text-align: left">Cenário Ideal</th>
          <th style="text-align: left">Vantagens</th>
          <th style="text-align: left">Desvantagens</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong><code>requests</code></strong></td>
          <td style="text-align: left">Obter o HTML de páginas estáticas.</td>
          <td style="text-align: left">Simples, leve, universal.</td>
          <td style="text-align: left">Não executa JavaScript.</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong><code>BeautifulSoup</code></strong></td>
          <td style="text-align: left">Extrair dados de HTML estático.</td>
          <td style="text-align: left">Sintaxe muito fácil e amigável.</td>
          <td style="text-align: left">Mais lento que <code>lxml</code>.</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong><code>lxml</code></strong></td>
          <td style="text-align: left">Extrair dados com alta velocidade.</td>
          <td style="text-align: left">Extremamente rápido, robusto, suporta XPath.</td>
          <td style="text-align: left">Sintaxe um pouco menos intuitiva que BS.</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong><code>Selenium</code></strong></td>
          <td style="text-align: left">Sites dinâmicos que exigem interação.</td>
          <td style="text-align: left">Executa JS, simula ações humanas.</td>
          <td style="text-align: left">Lento, consome mais recursos (abre um navegador).</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong><code>Scrapy</code></strong></td>
          <td style="text-align: left">Projetos grandes, crawling de sites inteiros.</td>
          <td style="text-align: left">Rápido (assíncrono), escalável, completo.</td>
          <td style="text-align: left">Curva de aprendizado maior, mais complexo para tarefas simples.</td>
      </tr>
  </tbody>
</table>
<hr>
<h1 id="dificuldades-que-podemos-ter">Dificuldades que podemos ter<a hidden class="anchor" aria-hidden="true" href="#dificuldades-que-podemos-ter">#</a></h1>
<p>Na prática, minerar sites de forma robusta e em larga escala é um desafio complexo. Os exemplos simples funcionam em ambientes ideais, mas o mundo real da web apresenta muitos obstáculos.</p>
<p>Alguns dos motivos são:</p>
<ol>
<li><code>Sites Dinâmicos</code> e <code>JavaScript</code>: Muitos sites modernos carregam seu conteúdo principal usando <code>JavaScript</code> após o carregamento inicial da página.</li>
</ol>
<ul>
<li><em>O Problema</em>: A biblioteca <code>requests</code> só consegue ver o <code>HTML</code> original enviado pelo servidor. Se o preço, o nome do produto ou o texto da notícia só aparece na tela após alguns segundos, o <code>requests</code> não o encontrará.</li>
<li><em>A Solução (mais complexa)</em>: É aqui que entra o <code>Selenium</code>. Ele precisa abrir um navegador real, esperar o <code>JavaScript</code> ser executado para renderizar a página completa e só então extrair a informação. Isso torna o processo muito mais lento e consome mais recursos.</li>
</ul>
<ol start="2">
<li>Tecnologias <code>Anti-Scraping</code>:
Os sites ativamente tentam impedir a mineração de dados para proteger suas informações, evitar sobrecarga no servidor e manter a vantagem competitiva.</li>
</ol>
<ul>
<li><em>Bloqueio por <code>IP</code> e Limite de Requisições (Rate Limiting):</em> Se um site detecta muitas requisições vindas do mesmo endereço de <code>IP</code> em um curto período, ele assume que é um robô e bloqueia o acesso temporária ou permanentemente.</li>
<li><em>Verificação de <code>User-Agent</code>:</em> O servidor verifica o <code>User-Agent</code> para saber quem está acessando (ex: &ldquo;<code>Chrome</code> no Windows&rdquo;, &ldquo;<code>Safari</code> no iPhone&rdquo;). Requisições de scripts <code>Python</code> têm um User-Agent padrão (ex: <code>python-requests/2.28.1</code>) que entrega imediatamente que é um bot. É preciso mascará-lo para se parecer com um navegador real.</li>
<li><em>CAPTCHAs:</em> O famoso &ldquo;Não sou um robô&rdquo;. Esses testes são projetados especificamente para serem fáceis para humanos e extremamente difíceis para robôs. Contorná-los é um desafio técnico avançado e eticamente questionável.</li>
<li><em>Honeypots (Potes de Mel):</em> São armadilhas para bots. Os desenvolvedores do site podem incluir links invisíveis para humanos, mas que um robô (que lê o <code>HTML</code> puro) seguiria. Ao acessar esse link, o <code>IP</code> do robô é imediatamente identificado e bloqueado.</li>
</ul>
<ol start="3">
<li>Estrutura do Site e Seletores Frágeis:
Seu código depende de encontrar elementos <code>HTML</code> por meio de seletores (ex: <code>find('div', class_='price')</code>).</li>
</ol>
<ul>
<li><em>O Problema:</em> Os sites mudam de layout o tempo todo. Uma simples atualização da equipe de desenvolvimento pode alterar o nome da classe de <code>&quot;price&quot;</code> para <code>&quot;product-price&quot;</code>, e seu scraper quebra instantaneamente.</li>
<li><em>A Solução (mais complexa):</em> Requer manutenção constante. É preciso criar seletores mais robustos e, às vezes, ter lógicas alternativas para encontrar o mesmo dado se o seletor principal falhar.</li>
</ul>
<ol start="4">
<li>Autenticação e Gerenciamento de Sessão:
Muitos dados valiosos estão por trás de uma tela de login (redes sociais, fóruns privados, áreas de clientes).</li>
</ol>
<ul>
<li><em>O Problema:</em> Não basta acessar a <code>URL</code>. Seu scraper precisa primeiro fazer login (enviando dados de um formulário POST) e depois gerenciar <code>cookies</code> e <code>tokens</code> de sessão para se manter autenticado nas requisições seguintes.</li>
<li><em>A Solução (mais complexa):</em> Utilizar objetos de sessão (<code>requests.Session</code>) para persistir os <code>cookies</code> ou automatizar o processo de login com o <code>Selenium</code>.</li>
</ul>
<ol start="5">
<li>Limpeza e Formatação dos Dados:
Os dados extraídos raramente vêm em um formato limpo e pronto para uso.</li>
</ol>
<ul>
<li><em>O Problema:</em> Você extrai um preço como <code>&quot;R$ 1.999,90\n &quot;</code>. Para poder usá-lo em cálculos, você precisa remover o <code>&quot;R$&quot;</code>, os espaços em branco, o caractere de nova linha <code>(\n)</code>, o ponto dos milhares e trocar a vírgula por um ponto para convertê-lo no número <code>1999.90</code>. Isso se aplica a datas, números de telefone, etc.</li>
<li><em>A Solução (mais complexa):</em> Requer uma etapa robusta de processamento e limpeza de dados (geralmente usando bibliotecas como <code>Pandas</code>) após a extração.</li>
</ul>
<hr>
<h1 id="boas-práticas">Boas práticas<a hidden class="anchor" aria-hidden="true" href="#boas-práticas">#</a></h1>
<p>Para garantir que a coleta de dados tenha uma boa qualidade, algumas práticas podem ser consideradas benéficas. Vamos citar algumas delas e ver seus impactos no desenvolvimento:</p>
<h2 id="pausas-entre-requests">Pausas entre requests<a hidden class="anchor" aria-hidden="true" href="#pausas-entre-requests">#</a></h2>
<p>As <strong>pausas entre requests</strong> (ou <code>request delays</code>) são uma das práticas mais importantes no web scraping, porque ajudam a manter o bot <strong>eficiente</strong> e menos propenso a ser <strong>bloqueado</strong>. Alguns motivos de se realizar essas pausas são:</p>
<ol>
<li><strong>Evitar sobrecarregar o servidor:</strong><br>
Quando o scraper faz requisições excessivas, é possível que isso cause lentidão no site ou, no pior dos casos, fazer com que o site caia.</li>
<li><strong>Respeitar limites:</strong><br>
Certos sites são rígidos com seus limites de acesso no <code>robots.txt</code>.</li>
<li><strong>Risco de bloqueio:</strong><br>
Sites geralmente têm mecanismos de defesa contra tráfego suspeito (<code>firewalls</code>, <code>rate limiting</code>, <code>bloqueio de IP</code>). Pausas tornam o acesso mais parecido com o de um usuário humano.</li>
</ol>
<p>Aqui está um exemplo de como essas pausas são implementadas, sendo, nesse caso, uma pausa fixa de 1 requisição a cada 5 segundos:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>import time
</span></span><span style="display:flex;"><span>import requests
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>urls <span style="color:#f92672">=</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;http://example.com/page1&#34;</span>, <span style="color:#e6db74">&#34;http://example.com/page2&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> url in urls:
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> requests.get<span style="color:#f92672">(</span>url<span style="color:#f92672">)</span>        <span style="color:#75715e"># resposta do processo</span>
</span></span><span style="display:flex;"><span>    time.sleep<span style="color:#f92672">(</span>5<span style="color:#f92672">)</span>                       <span style="color:#75715e"># Espera por 5 segundos</span>
</span></span></code></pre></div><h2 id="salvar-durante-execução">Salvar durante execução<a hidden class="anchor" aria-hidden="true" href="#salvar-durante-execução">#</a></h2>
<p>Um forte aliado das pausas entre requests é salvar seu progresso durante a execução da coleta, ou seja, <strong>salvar os dados gradualmente enquanto o scraper roda</strong>, em vez de deixar tudo para o final. Isso traz algumas vantagens como a <strong>eficiência</strong> e a <strong>segurança dos dados</strong>. Salvar, mesmo que de pouco em pouco, pode ser importante já que <strong>evita a perda de dados</strong> se erros ocorrerem durante o processo de coleta de dados, não se perderia nada muito impactante para o projeto.</p>
<p>Para salvar o progresso gradualmente, algumas técnicas podem ser implementadas, como:</p>
<ol>
<li><strong>Escrita incremental em arquivos:</strong></li>
</ol>
<ul>
<li>Podemos gerar um arquivo <code>CSV/JSON</code> em que, a cada item extraido, o mesmo é incrementado no arquivo, por exemplo:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>import csv
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>with open<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;noticias.csv&#34;</span>, <span style="color:#e6db74">&#34;a&#34;</span>, newline<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">)</span> as f:
</span></span><span style="display:flex;"><span>    writer <span style="color:#f92672">=</span> csv.writer<span style="color:#f92672">(</span>f<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> noticia in noticias:
</span></span><span style="display:flex;"><span>        writer.writerow<span style="color:#f92672">([</span>noticia<span style="color:#f92672">[</span><span style="color:#e6db74">&#34;titulo&#34;</span><span style="color:#f92672">]</span>, noticia<span style="color:#f92672">[</span><span style="color:#e6db74">&#34;link&#34;</span><span style="color:#f92672">]</span>, noticia<span style="color:#f92672">[</span><span style="color:#e6db74">&#34;data&#34;</span><span style="color:#f92672">]])</span>
</span></span></code></pre></div><ul>
<li>Essa técnica, por mais que seja símples, pode gerar arquivos muito grandes.</li>
</ul>
<ol start="2">
<li><strong>Escrita diretamente no banco de dados:</strong></li>
</ol>
<ul>
<li>Inserir dados conforme forem extraidos é bom para scrapers que rodam em um loop contínuo.</li>
<li>Utilizando um banco de dados como <code>MongoDB</code>, que armazena JSON nativamente é ideal, já que scrapers extraem dados em dicionários, além de ser <strong>flexível</strong> e ter uma ótima <strong>escalabilidade</strong>.</li>
<li>Exemplo utilizando MongoDB:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> n in noticias:
</span></span><span style="display:flex;"><span>    titulo <span style="color:#f92672">=</span> n.get_text<span style="color:#f92672">(</span>strip<span style="color:#f92672">=</span>True<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    link <span style="color:#f92672">=</span> n.find_parent<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;a&#34;</span><span style="color:#f92672">)[</span><span style="color:#e6db74">&#34;href&#34;</span><span style="color:#f92672">]</span> <span style="color:#66d9ef">if</span> n.find_parent<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;a&#34;</span><span style="color:#f92672">)</span> <span style="color:#66d9ef">else</span> None
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    noticia <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;titulo&#34;</span>: titulo,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;link&#34;</span>: link,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;fonte&#34;</span>: <span style="color:#e6db74">&#34;G1&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># insere imediatamente no banco</span>
</span></span><span style="display:flex;"><span>    colecao.insert_one<span style="color:#f92672">(</span>noticia<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    print<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Salvo:&#34;</span>, titulo<span style="color:#f92672">)</span>
</span></span></code></pre></div><ol start="3">
<li><strong>Scrapy pipelines:</strong></li>
</ol>
<ul>
<li>O scrapy já implementa essa prática de salvamento do progresso nativamente. Cada item coletado passa automaticamente por uma <code>pipeline de dados</code>, onde você decide como salvar (CSV, JSON, banco de dados etc).</li>
</ul>
<h1 id="fontes">Fontes:<a hidden class="anchor" aria-hidden="true" href="#fontes">#</a></h1>
<ul>
<li><a href="https://github.com/aosfatos/check-up?tab=readme-ov-file#3--collect-ad-information">Repositorio &ldquo;check up&rdquo;</a></li>
<li><a href="https://kinsta.com/pt/base-de-conhecimento/o-que-e-web-scraping/">kinsta - o que é web scraping</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/2025-2-SeLiga/estudos/protocolo_HTTP/">
    <span class="title">« Prev</span>
    <br>
    <span>Protocolo HTTP (Hypertext Transfer Protocol)</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/2025-2-SeLiga/">SeLiga</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
